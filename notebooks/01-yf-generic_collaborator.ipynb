{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from numba import njit\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "from abc import abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collaborator:\n",
    "    \"\"\"\n",
    "    An abstract class representing a collaborator.\n",
    "    Collaborators can be of different types (running with different algorithms)\n",
    "    e.g. Gtms, gaussian mixture models, other mixture models, or any kind of probabilistic model.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    @njit\n",
    "    def __init__(self, data_Id:np.array, X:np.array, Y=None, K=3, use_criterion=False, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        data_Id: np.array(int)\n",
    "            N-dimensional array, Id of each individual.\n",
    "        X: np.ndarray(float)\n",
    "            N*D array of features.\n",
    "        use_criterion: bool\n",
    "            Whether to use criterion to accept collaboration or not. Default to False.\n",
    "            \n",
    "        Optional:\n",
    "        Y : np.array(int)\n",
    "            N-dimensional array, labels. Default to None.\n",
    "        K : int\n",
    "            Number of clusters. Default to 3.\n",
    "        add_noise: bool\n",
    "            Whether to add white noise or not. Default to False.\n",
    "        noise_scale: float\n",
    "            If add_noise==True, the variance of the noise in each dimension is equal to this value\n",
    "            multiplied by the variance of the data. Default to 0.1.\n",
    "            \n",
    "        define:\n",
    "        self.Id: int\n",
    "            Id of the collaborator. Set by the setter method.\n",
    "        self.K: int\n",
    "            The number of clusters.\n",
    "        self.N: int\n",
    "            Number of lines in the dataset.\n",
    "        self.D: int\n",
    "            Dimensionality of the data.\n",
    "        self.data_Id: np.array(int)\n",
    "            N-dimensional array, Id of each individual.\n",
    "        self.X: np.array\n",
    "        self.R: np.array\n",
    "            Partition matrix.\n",
    "        self.history_R: list(np.array)\n",
    "            List of all partition matrices.\n",
    "        self.H: np.array\n",
    "            N-dimensional array. Entropy of the classification of each individual.\n",
    "        self.use_criterion: bool\n",
    "            Whether to use criterion to accept collaboration or not. Default to False.\n",
    "        self.criterion: float\n",
    "            Current value of the criterion used to decide whether to accept collaboration.\n",
    "            Computed with self.get_criterion()\n",
    "        \"\"\"\n",
    "        \n",
    "        self.Id = None # set by the master algorithm using self.set_id\n",
    "        self.K = K\n",
    "        self.N, self.D= None, None\n",
    "        self.data_Id, self.X = data_Id, self.parseX(X, **kwargs)\n",
    "        if Y:\n",
    "            self.Y = deepcopy(Y)\n",
    "        self.R = np.zeros((N, K))\n",
    "        self.history_R = []\n",
    "        self.H = np.zeros(N)\n",
    "        self.use_criterion, self.criterion = use_criterion, None\n",
    "        self.db, self.purity, self.silhouette = None, None, None\n",
    "        \n",
    "            \n",
    "    \n",
    "    @njit\n",
    "    def parseX(self, X, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        parse the dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        res = deepcopy(X)\n",
    "        \n",
    "        # we want a 2-D array\n",
    "        if res.ndim == 1:\n",
    "            res = res.reshape(-1, 1)\n",
    "        self.N, self.D = res.shape\n",
    "        \n",
    "        # If add_noise is set to True, then add noise\n",
    "        if kwargs.get('add_noise', False):\n",
    "            std = np.std(res, axis=0)\n",
    "            noise_std = kwargs.get('noise_scale', .1) * np.diag(std)\n",
    "            noise = scipy.random.multivariate_normal(mean=np.zeros(self.D), cov=noise_std, size=self.N)\n",
    "        res += noise\n",
    "        \n",
    "        return res\n",
    "        \n",
    "        \n",
    "    @abstractmethod\n",
    "    def local_step(self):\n",
    "        \"\"\"\n",
    "        Fit the parameters to the dataset.\n",
    "        Add first partition matrix to history.\n",
    "        Also initialize the validation indices, and in particular the criterion.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def log_local(self):\n",
    "        \"\"\"\n",
    "        First log, after local step. Log the values of the various validation indices (db, purity, silhouette).\n",
    "        \n",
    "        Returns:\n",
    "            log: dict\n",
    "            Dictionary with the values to log: at least the validation indices (db, purity, silhouette).\n",
    "        \"\"\"\n",
    "        \n",
    "        self.db = self.compute_db()\n",
    "        self.purity = self.compute_purity()\n",
    "        self.silhouette = self.compute_silhouette()\n",
    "        \n",
    "        res = {\n",
    "            \"db\":self.db,\n",
    "            \"purity\":self.purity,\n",
    "            \"silhouette\":self.silhouette\n",
    "        }\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        add Normalized Mutual Information.\n",
    "        \"\"\"\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    \n",
    "    @njit\n",
    "    def collaborate(self, remote_Ids, remote_Rs): # horizontal collab for now\n",
    "        \"\"\"\n",
    "        Compute a new collaboration matrix, given all remote matrices.\n",
    "        \n",
    "        Parameters:\n",
    "        remote_Ids: list(int)\n",
    "            List of Ids of the collaborators.\n",
    "        remote_Rs: list(np.array)\n",
    "            List of P N*K(p)-dimensional arrays.\n",
    "            Where K(p) is the number of clusters in data site number p (p=1,...,P).\n",
    "            \n",
    "        returns:\n",
    "            res: np.array\n",
    "                N*K array, the collaborated partition matrix.\n",
    "            confidence_coefficients: np.array\n",
    "                P-dimensional array. The confidence coefficient of collab with each remote site.\n",
    "        \"\"\"\n",
    "        \n",
    "        # number of collaborators\n",
    "        P = len(remote_Rs) + 1\n",
    "        \n",
    "        # vector of confidence coefficients.\n",
    "        confidence_coefficients = np.zeros(P)\n",
    "        \n",
    "        # res\n",
    "        res = np.zeros_like(self.R)\n",
    "        \n",
    "        # entropy of local classification\n",
    "        local_H = self.compute_entropy(self.R)\n",
    "            \n",
    "        for p, (remote_Id, remote_R) in enumerate(zip(remote_Ids, remote_Rs)):\n",
    "            # optimal transport\n",
    "            remote_R = self.optimal_transport(remote_R)\n",
    "            remote_H = self.compute_entropy(remote_R)\n",
    "            # compute the local and remote coefficients (one coeff for each individual)\n",
    "            l, r = (1/(P-1)) * remote_H * (1-local_H), local_H * (1-remote_H)\n",
    "            res += l * self.R + r * remote_R\n",
    "            # update confidence vector\n",
    "            confidence_coefficients[remote_Id] += r.sum()\n",
    "            confidence_coefficients[self.Id] += l.sum()\n",
    "            \n",
    "        # normalize partition matrix\n",
    "        res /= res.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        # decide whether to accept collaboration\n",
    "        if self.use_criterion:\n",
    "            self.refit(res)\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "    @njit\n",
    "    def future_collaborate(self, remote_Ids, remote_data_Ids, remote_Rs): # horizontal collab for now\n",
    "        \"\"\"\n",
    "        Compute a new collaboration matrix, given all remote matrices.\n",
    "        \n",
    "        Parameters:\n",
    "        remote_Ids: list(int)\n",
    "            List of Ids of the collaborators.\n",
    "        remote_data_Ids: list(np.array)\n",
    "            List of P N(p)-dimensional arrays, Id of each individual.\n",
    "            Where N(p) is the number of observations available on data site number p (p=1,...,P).\n",
    "        remote_Rs: list(np.array)\n",
    "            List of P N(p)*K(p)-dimensional arrays.\n",
    "            Where K(p) is the number of clusters in data site number p (p=1,...,P).\n",
    "            \n",
    "        returns:\n",
    "            The collaborated partition matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        # number of collaborators\n",
    "        P = len(remote_Rs) + 1\n",
    "        \n",
    "        # Ids and partition matrices after left join.\n",
    "        lj_remote_data_Ids, lj_remote_Rs = [], []\n",
    "        # partition matrices after optimal transport.\n",
    "        transported_remote_Rs = []\n",
    "        # vector of confidence coefficients.\n",
    "        confidence_coefficient = np.zeros(P)\n",
    "        \n",
    "        #local_component, remote_component = np.zeros_like(self.R), np.zeros_like(self.R)\n",
    "        \n",
    "        # we need to know, for each individual, how many data sites have it.\n",
    "        count = np.zeros_like(self.data_Id)\n",
    "        \n",
    "        for p, (remote_data_Id, remote_R) in enumerate(zip(remote_data_Ids, remote_Rs)):\n",
    "            # keep only common observations\n",
    "            remote_data_Id, remote_R = self.left_join(remote_data_Id, remote_R)\n",
    "            lj_remote_data_Ids.append(deepcopy(remote_data_Id))\n",
    "            lj_remote_Rs.append(deepcopy(remote_R))\n",
    "            # update the count variable\n",
    "            count += self.find_indices(self.data_Id, lj_remote_data_Id)\n",
    "            \n",
    "            \n",
    "        \n",
    "        for p, (remote_data_Id, remote_R) in enumerate(zip(remote_data_Ids, remote_Rs)):\n",
    "            \n",
    "            \n",
    "            # optimal transport\n",
    "            remote_R = self.optimal_transport(remote_R)\n",
    "            remote_H = self.compute_entropy(remote_R)\n",
    "            # compute the local and remote coefficients (one coeff for each individual)\n",
    "            l, r = remote_H * (1-self.H), self.H * (1-remote_H)\n",
    "            local_component += l\n",
    "            remote_component += r\n",
    "            \n",
    "    \n",
    "    @abstractmethod\n",
    "    def log_collab(self):\n",
    "        \"\"\"\n",
    "        Log the results of a collaboration step:\n",
    "        the validation indices (db, purity, silhouette) and the confidence vector.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def get_partition_matrix(self):\n",
    "        \"\"\"\n",
    "        Accessor. Returns the partition matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.R\n",
    "    \n",
    "    \n",
    "    def set_id(self, Id):\n",
    "        \"\"\"\n",
    "        Mutator\n",
    "        \"\"\"\n",
    "        self.Id = Id\n",
    "        \n",
    "        \n",
    "    def get_id(self):\n",
    "        \"\"\"\n",
    "        Accessor\n",
    "        \"\"\"\n",
    "        return self.Id\n",
    "    \n",
    "        \n",
    "    def compute_db(self, resp=None):\n",
    "        \"\"\"\n",
    "        compute the DB index of a dataset, given a clustering for this dataset\n",
    "\n",
    "        Args:\n",
    "            resp: array-like, (n_samples, n_clusters)\n",
    "                reponsibility matrix\n",
    "\n",
    "        Returns:\n",
    "            float, the DB index\n",
    "        \"\"\"\n",
    "\n",
    "        resp = resp if resp is not None else self.R\n",
    "        \n",
    "        try:\n",
    "            # a hard partition is required\n",
    "            y_pred = resp.argmax(axis=1)\n",
    "\n",
    "            return metrics.davies_bouldin_score(self.X, y_pred)\n",
    "\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    def compute_purity(self, y_true=None, y_pred=None):\n",
    "        \"\"\"\n",
    "        compute the purity score of a clustering\n",
    "\n",
    "        Args:\n",
    "            y_true: array-like, (n_samples,)\n",
    "                labels of each observation\n",
    "            y_pred: array-like, (n_samples,) or (n_samples, n_clusters)\n",
    "                predicted hard clustering\n",
    "\n",
    "        Returns: float\n",
    "                purity score.\n",
    "        \"\"\"\n",
    "        \n",
    "        # if we do not have the labels, return None.\n",
    "        if y_true == None:\n",
    "            return None\n",
    "        \n",
    "        y_pred = y_pred if y_pred is not None else self.R\n",
    "        if y_pred.ndim == 2:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # compute contingency matrix (also called confusion matrix).\n",
    "        contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "\n",
    "        return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "    \n",
    "    \n",
    "    def compute_silhouette(self, y_pred=None):\n",
    "        \"\"\"\n",
    "        Compute the silhouette index of the classification.\n",
    "        Args:\n",
    "            y_pred: array-like, (n_samples,) or (n_samples, n_clusters)\n",
    "                predicted hard clustering or partition matrix.\n",
    "            \n",
    "        Returns: float\n",
    "            silhouette index.\n",
    "        \"\"\"\n",
    "        \n",
    "        y_pred = y_pred if y_pred is not None else self.R\n",
    "        if y_pred.ndim == 2:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "        try:\n",
    "            return metrics.silhouette_score(self.X, y_pred)\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test function to find indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "Id = np.array([3, 5, 1, 7, 9])\n",
    "count = np.zeros_like(Id)\n",
    "present = np.array([1, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(Id, present):\n",
    "    res = np.zeros_like(Id)\n",
    "    for x in present:\n",
    "        res[np.where(Id==x)]+=1\n",
    "    return res\n",
    "\n",
    "@njit\n",
    "def fast_find_indices(Id, present):\n",
    "    res = np.zeros_like(Id)\n",
    "    for x in present:\n",
    "        res[np.where(Id==x)]+=1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.06 µs ± 270 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "find_indices(Id, present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772 ns ± 12.1 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "fast_find_indices(Id, present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_list:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.values = []\n",
    "        for e in args:\n",
    "            self.values.append(e)\n",
    "            \n",
    "    def __eq__(self, other):\n",
    "        try:\n",
    "            assert isinstance(other, my_list)\n",
    "        except:\n",
    "            print(\"only compare to another instance of my_list\")\n",
    "            return False\n",
    "        if len(self.values) != len(other.values):\n",
    "            print(\"length difference: {} != {}\".format(len(self.values), len(other.values)))\n",
    "            return False\n",
    "        for s, o in zip(self.values, other.values):\n",
    "            if s != o:\n",
    "                print(\"{} != {}\".format(s, o))\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = my_list(1, 2)\n",
    "b = my_list(1, 2)\n",
    "c = my_list(1, 2, 3)\n",
    "d = [1, 2]\n",
    "e = my_list(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a==b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ijcnn21",
   "language": "python",
   "name": "ijcnn21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
