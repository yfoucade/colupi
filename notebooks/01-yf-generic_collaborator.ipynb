{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ot\n",
    "import scipy\n",
    "\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "\n",
    "from numba import njit\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "from abc import abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collaborator:\n",
    "    \"\"\"\n",
    "    An abstract class representing a collaborator.\n",
    "    Collaborators can be of different types (running with different algorithms)\n",
    "    e.g. Gtms, gaussian mixture models, other mixture models, or any kind of probabilistic model.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, data_Id:np.array, X:np.array, Y=None, K=3, use_criterion=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        data_Id: np.array(int)\n",
    "            N-dimensional array, Id of each individual.\n",
    "        X: np.ndarray(float)\n",
    "            N*D array of features.\n",
    "        use_criterion: bool\n",
    "            Whether to use criterion to accept collaboration or not. Default to False.\n",
    "            \n",
    "        Optional:\n",
    "        Y : np.array(int)\n",
    "            N-dimensional array, labels. Default to None.\n",
    "        K : int\n",
    "            Number of clusters. Default to 3.\n",
    "        add_noise: bool\n",
    "            Whether to add white noise or not. Default to False.\n",
    "        noise_scale: float\n",
    "            If add_noise==True, the variance of the noise in each dimension is equal to this value\n",
    "            multiplied by the variance of the data. Default to 0.1.\n",
    "            \n",
    "        define:\n",
    "        self.Id: int\n",
    "            Id of the collaborator. Set by the setter method.\n",
    "        self.K: int\n",
    "            The number of clusters.\n",
    "        self.N: int\n",
    "            Number of lines in the dataset.\n",
    "        self.D: int\n",
    "            Dimensionality of the data.\n",
    "        self.data_Id: np.array(int)\n",
    "            N-dimensional array, Id of each individual.\n",
    "        self.X: np.array\n",
    "        self.R: np.array\n",
    "            Partition matrix.\n",
    "        self.history_R: list(np.array)\n",
    "            List of all partition matrices.\n",
    "        self.H: np.array\n",
    "            N-dimensional array. Entropy of the classification of each individual.\n",
    "        self.use_criterion: str\n",
    "            If not None, one of 'db', 'purity', 'silhouette'\n",
    "        self.criterion: float\n",
    "            Current value of the criterion used to decide whether to accept collaboration.\n",
    "            Computed with self.get_criterion()\n",
    "        \"\"\"\n",
    "        \n",
    "        self.Id = None # set by the master algorithm using self.set_id\n",
    "        self.K = K\n",
    "        self.N, self.D= None, None\n",
    "        self.data_Id, self.X = data_Id, self.parseX(X, **kwargs)\n",
    "        if Y:\n",
    "            self.Y = deepcopy(Y)\n",
    "        self.R = np.zeros((N, K))\n",
    "        self.history_R = []\n",
    "        self.H = np.zeros(N)\n",
    "        self.use_criterion, self.criterion = use_criterion, None\n",
    "        self.validation_indices_history = []\n",
    "        self.confidence_coefficients_history = []\n",
    "        \n",
    "            \n",
    "    \n",
    "    def parseX(self, X, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        parse the dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        res = deepcopy(X)\n",
    "        \n",
    "        # we want a 2-D array\n",
    "        if res.ndim == 1:\n",
    "            res = res.reshape(-1, 1)\n",
    "        self.N, self.D = res.shape\n",
    "        \n",
    "        # If add_noise is set to True, then add noise\n",
    "        if kwargs.get('add_noise', False):\n",
    "            std = np.std(res, axis=0)\n",
    "            noise_std = kwargs.get('noise_scale', .1) * np.diag(std)\n",
    "            noise = scipy.random.multivariate_normal(mean=np.zeros(self.D), cov=noise_std, size=self.N)\n",
    "        res += noise\n",
    "        \n",
    "        return res\n",
    "        \n",
    "        \n",
    "    @abstractmethod\n",
    "    def local_step(self):\n",
    "        \"\"\"\n",
    "        Fit the parameters to the dataset.\n",
    "        Add first partition matrix to history.\n",
    "        Also initialize the validation indices, and in particular the criterion \n",
    "        (self.criterion = self.validation_indices_history[-1][self.use_criterion])\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def refit(self, R):\n",
    "        \"\"\"\n",
    "        Fit the parameters of the model starting from matrix R.\n",
    "        \n",
    "        Parameters:\n",
    "            R: np.ndarray\n",
    "                N*K array, responsibility matrix.\n",
    "                \n",
    "        Returns:\n",
    "            A list, the first elements are the fitted parameters of the model.\n",
    "            The last element is a dictionary with the validation criteria (at leats db index and silhouette).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def log_local(self):\n",
    "        \"\"\"\n",
    "        First log, after local step. Log the values of the various validation indices (db, purity, silhouette).\n",
    "        \n",
    "        Returns:\n",
    "            log: dict\n",
    "            Dictionary with the values to log: at least the validation indices (db, purity, silhouette).\n",
    "        \"\"\"\n",
    "        \n",
    "        self.db = self.compute_db()\n",
    "        self.purity = self.compute_purity()\n",
    "        self.silhouette = self.compute_silhouette()\n",
    "        \n",
    "        res = {\n",
    "            \"db\":self.db,\n",
    "            \"purity\":self.purity,\n",
    "            \"silhouette\":self.silhouette\n",
    "        }\n",
    "        \n",
    "        return res\n",
    "    \n",
    "        \"\"\"\n",
    "        or simply:\n",
    "        return self.validation_indices_history[0]\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        add Normalized Mutual Information.\n",
    "        \"\"\"\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def collaborate(self, remote_Ids, remote_Rs): # horizontal collab for now\n",
    "        \"\"\"\n",
    "        Compute a new collaboration matrix, given all remote matrices.\n",
    "        \n",
    "        Parameters:\n",
    "        remote_Ids: list(int)\n",
    "            List of Ids of the collaborators.\n",
    "        remote_Rs: list(np.array)\n",
    "            List of P N*K(p)-dimensional arrays.\n",
    "            Where K(p) is the number of clusters in data site number p (p=1,...,P).\n",
    "            \n",
    "        returns:\n",
    "            res: np.array\n",
    "                N*K array, the collaborated partition matrix.\n",
    "            confidence_coefficients: np.array\n",
    "                P-dimensional array. The confidence coefficient of collab with each remote site.\n",
    "        \"\"\"\n",
    "        \n",
    "        # number of collaborators\n",
    "        P = len(remote_Rs) + 1\n",
    "        \n",
    "        # vector of confidence coefficients.\n",
    "        confidence_coefficients = np.zeros(P)\n",
    "        \n",
    "        # res\n",
    "        res = np.zeros_like(self.R)\n",
    "        \n",
    "        # entropy of local classification\n",
    "        local_H = self.compute_entropy(self.R)\n",
    "            \n",
    "        for p, (remote_Id, remote_R) in enumerate(zip(remote_Ids, remote_Rs)):\n",
    "            # optimal transport\n",
    "            remote_R = self.optimal_transport(remote_R)\n",
    "            remote_H = self.compute_entropy(remote_R)\n",
    "            # compute the local and remote coefficients (one coeff for each individual)\n",
    "            l, r = (1/(P-1)) * remote_H * (1-local_H), local_H * (1-remote_H)\n",
    "            res += l * self.R + r * remote_R\n",
    "            # update confidence vector\n",
    "            confidence_coefficients[remote_Id] += r.sum()\n",
    "            confidence_coefficients[self.Id] += l.sum()\n",
    "            \n",
    "        # normalize partition matrix\n",
    "        res /= res.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        # decide whether to accept collaboration\n",
    "        update = True\n",
    "        params_and_indices = self.refit(deepcopy(res))\n",
    "        params, indices = params_and_indices[:-1], params_and_indices[-1]\n",
    "        if self.use_criterion:\n",
    "            update = True if compare_criterion(indices) else False\n",
    "            \n",
    "        if update:\n",
    "            successful_collab = True\n",
    "            self.save_new_values(res, params, indices)\n",
    "        else:\n",
    "            successful_collab = False\n",
    "            confidence_coefficients = np.zeros(P)\n",
    "            self.save_old_values()\n",
    "            \n",
    "        self.confidence_coefficients_history.append(confidence_coefficients)\n",
    "        return successful_collab\n",
    "    \n",
    "        \n",
    "    def save_new_values(self, R, params, indices):\n",
    "        \"\"\"\n",
    "        save the new values in history\n",
    "        \"\"\"\n",
    "        \n",
    "        self.R = R\n",
    "        self.history_R.append(deepcopy(R))\n",
    "        self.params = params\n",
    "        self.indices = indices\n",
    "        \n",
    "            \n",
    "    def save_old_values(self, R, params, indices):\n",
    "        \"\"\"\n",
    "        no collaboration: save the old values\n",
    "        \"\"\"\n",
    "        \n",
    "        self.history_R.append(self.history_R[-1])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def optimal_transport(self, local_R=None, remote_R):\n",
    "        \"\"\"\n",
    "        compute the optimal transport plan between the remote partition matrix remote_R and the\n",
    "        local one.\n",
    "        Returns the transported remote partition in the local space.\n",
    "        \"\"\"\n",
    "        \n",
    "        if local_R is None:\n",
    "            local_R = self.R\n",
    "            \n",
    "        # compute the mass distribution (weight of each cluster)\n",
    "        local_w = self.local_R.sum(axis=0)/local_R.sum()\n",
    "        remote_w = remote_R.sum(axis=0)/remote_R.sum()\n",
    "        \n",
    "        # compute the cost matrix\n",
    "        M = sk.metrics.pairwise_distances(local_R, remote_R)\n",
    "        \n",
    "        # compute the optimal transport plan\n",
    "        gamma = ot.lp.emd(local_w, remote_w, M)\n",
    "        \n",
    "        # transport\n",
    "        res = np.dot(remote_R, gamma.T)/np.dot(remote_R, gamma.T).sum(axis=1, keepdims=True)\n",
    "        \n",
    "        return res\n",
    "    \n",
    "        \n",
    "    def compute_entropy(self, R=None):\n",
    "        \"\"\"\n",
    "        compute normalized entropy\n",
    "\n",
    "        Args:\n",
    "            tau: ndarray\n",
    "                the probabilistic partition matrix\n",
    "\n",
    "        Returns:\n",
    "            H: ndarray\n",
    "                N-dimensional vector: entropy for each data point.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        K = R.shape[1]\n",
    "\n",
    "        # compute pairwise entropies\n",
    "        pairwise_entropy = -R.dot(np.log2(R, where = R>0).T)\n",
    "        # normalize, maximum entropy is given by uniform distribution over K\n",
    "        pairwise_entropy /= np.log2(K)\n",
    "\n",
    "        H = pairwise_entropy.diagonal().reshape(-1, 1)\n",
    "\n",
    "        return H\n",
    "        \n",
    "            \n",
    "    def future_collaborate(self, remote_Ids, remote_data_Ids, remote_Rs): # horizontal collab for now\n",
    "        \"\"\"\n",
    "        Compute a new collaboration matrix, given all remote matrices.\n",
    "        \n",
    "        Parameters:\n",
    "        remote_Ids: list(int)\n",
    "            List of Ids of the collaborators.\n",
    "        remote_data_Ids: list(np.array)\n",
    "            List of P N(p)-dimensional arrays, Id of each individual.\n",
    "            Where N(p) is the number of observations available on data site number p (p=1,...,P).\n",
    "        remote_Rs: list(np.array)\n",
    "            List of P N(p)*K(p)-dimensional arrays.\n",
    "            Where K(p) is the number of clusters in data site number p (p=1,...,P).\n",
    "            \n",
    "        returns:\n",
    "            The collaborated partition matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        # number of collaborators\n",
    "        P = len(remote_Rs) + 1\n",
    "        \n",
    "        # Ids and partition matrices after left join.\n",
    "        lj_remote_data_Ids, lj_remote_Rs = [], []\n",
    "        # partition matrices after optimal transport.\n",
    "        transported_remote_Rs = []\n",
    "        # vector of confidence coefficients.\n",
    "        confidence_coefficients = np.zeros(P)\n",
    "        \n",
    "        #local_component, remote_component = np.zeros_like(self.R), np.zeros_like(self.R)\n",
    "        \n",
    "        # we need to know, for each individual, how many data sites have it.\n",
    "        count = np.zeros_like(self.data_Id)\n",
    "        \n",
    "        for p, (remote_data_Id, remote_R) in enumerate(zip(remote_data_Ids, remote_Rs)):\n",
    "            # keep only common observations\n",
    "            remote_data_Id, remote_R = self.left_join(remote_data_Id, remote_R)\n",
    "            lj_remote_data_Ids.append(deepcopy(remote_data_Id))\n",
    "            lj_remote_Rs.append(deepcopy(remote_R))\n",
    "            # update the count variable\n",
    "            count += self.find_indices(self.data_Id, lj_remote_data_Id)\n",
    "            \n",
    "            \n",
    "        \n",
    "        for p, (remote_data_Id, remote_R) in enumerate(zip(remote_data_Ids, remote_Rs)):\n",
    "            \n",
    "            \n",
    "            # optimal transport\n",
    "            remote_R = self.optimal_transport(remote_R)\n",
    "            remote_H = self.compute_entropy(remote_R)\n",
    "            # compute the local and remote coefficients (one coeff for each individual)\n",
    "            l, r = remote_H * (1-self.H), self.H * (1-remote_H)\n",
    "            local_component += l\n",
    "            remote_component += r\n",
    "            \n",
    "    \n",
    "    @abstractmethod\n",
    "    def log_collab(self):\n",
    "        \"\"\"\n",
    "        Log the results of a collaboration step:\n",
    "        the validation indices (db, purity, silhouette) and the confidence vector.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def get_partition_matrix(self):\n",
    "        \"\"\"\n",
    "        Accessor. Returns the partition matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.R\n",
    "    \n",
    "    \n",
    "    def set_id(self, Id):\n",
    "        \"\"\"\n",
    "        Mutator\n",
    "        \"\"\"\n",
    "        self.Id = Id\n",
    "        \n",
    "        \n",
    "    def get_id(self):\n",
    "        \"\"\"\n",
    "        Accessor\n",
    "        \"\"\"\n",
    "        return self.Id\n",
    "    \n",
    "        \n",
    "    def compute_db(self, resp=None):\n",
    "        \"\"\"\n",
    "        compute the DB index of a dataset, given a clustering for this dataset\n",
    "\n",
    "        Args:\n",
    "            resp: array-like, (n_samples, n_clusters)\n",
    "                reponsibility matrix\n",
    "\n",
    "        Returns:\n",
    "            float, the DB index\n",
    "        \"\"\"\n",
    "\n",
    "        resp = resp if resp is not None else self.R\n",
    "        \n",
    "        try:\n",
    "            # a hard partition is required\n",
    "            y_pred = resp.argmax(axis=1)\n",
    "\n",
    "            return metrics.davies_bouldin_score(self.X, y_pred)\n",
    "\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    def compute_purity(self, y_true=None, y_pred=None):\n",
    "        \"\"\"\n",
    "        compute the purity score of a clustering\n",
    "\n",
    "        Args:\n",
    "            y_true: array-like, (n_samples,)\n",
    "                labels of each observation\n",
    "            y_pred: array-like, (n_samples,) or (n_samples, n_clusters)\n",
    "                predicted hard clustering\n",
    "\n",
    "        Returns: float\n",
    "                purity score.\n",
    "        \"\"\"\n",
    "        \n",
    "        # if we do not have the labels, return None.\n",
    "        if y_true == None:\n",
    "            return None\n",
    "        \n",
    "        y_pred = y_pred if y_pred is not None else self.R\n",
    "        if y_pred.ndim == 2:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # compute contingency matrix (also called confusion matrix).\n",
    "        contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "\n",
    "        return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "    \n",
    "    \n",
    "    def compute_silhouette(self, y_pred=None):\n",
    "        \"\"\"\n",
    "        Compute the silhouette index of the classification.\n",
    "        Args:\n",
    "            y_pred: array-like, (n_samples,) or (n_samples, n_clusters)\n",
    "                predicted hard clustering or partition matrix.\n",
    "            \n",
    "        Returns: float\n",
    "            silhouette index.\n",
    "        \"\"\"\n",
    "        \n",
    "        y_pred = y_pred if y_pred is not None else self.R\n",
    "        if y_pred.ndim == 2:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "        try:\n",
    "            return metrics.silhouette_score(self.X, y_pred)\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ijcnn21",
   "language": "python",
   "name": "ijcnn21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
